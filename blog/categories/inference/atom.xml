<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: inference | jtobin.ca]]></title>
  <link href="http://jtobin.ca/blog/categories/inference/atom.xml" rel="self"/>
  <link href="http://jtobin.ca/"/>
  <updated>2014-07-07T23:48:43-02:30</updated>
  <id>http://jtobin.ca/</id>
  <author>
    <name><![CDATA[Jared Tobin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Automasymbolic Differentiation]]></title>
    <link href="http://jtobin.ca/blog/2014/07/06/automasymbolic-differentiation/"/>
    <updated>2014-07-06T21:38:55-02:30</updated>
    <id>http://jtobin.ca/blog/2014/07/06/automasymbolic-differentiation</id>
    <content type="html"><![CDATA[<p>Automatic differentiation is one of those things that’s famous for not being as
famous as it should be (uh..).  It’s useful, it’s convenient, and yet fewer
know about it than one would think.  <a href="http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/">This article</a>
(by one of the guys working on
<a href="http://probcomp.csail.mit.edu/venture/">Venture</a>) is the single best
introduction you’ll probably find to AD, anywhere.  It gives a wonderful
introduction to the basics, the subtleties, and the gotchas of the subject.
You should read it.  I’m going to assume you have.</p>

<p>In particular, you should note this part:</p>

<blockquote>
  <p>[computing gradients] can be done — the method is called <em>reverse mode</em> — but
it introduces both code complexity and runtime cost in the form of managing
this storage (traditionally called the “tape”). In particular, the space
requirements of raw keverse mode are proportional to the <em>runtime</em> of f.</p>
</blockquote>

<p>In some applications this can be somewhat inconvenient - picture iterative
gradient-based sampling algorithms like <a href="http://arxiv.org/pdf/1206.1901.pdf">Hamiltonian Monte Carlo</a>, its famous auto-tuning version
<a href="http://arxiv.org/abs/1111.4246">NUTS</a>, and <a href="http://arxiv.org/pdf/1011.0057.pdf">Riemannian manifold variants</a>.  <a href="http://bayeshive.com/">Tom</a>
noted in <a href="http://www.reddit.com/r/haskell/comments/1odmk1/backpropogation_steepest_descent_and_automatic/ccrc6pg">this reddit thread</a>
that symbolic differentiation - which doesn’t need to deal with tapes and
infinitesimal-tracking - can often be orders of magnitude faster than AD for
this kind of problem.  When running these algos we calculate gradients many,
many times, and the additional runtime cost of the reverse-mode AD dance can
add up.</p>

<p>An interesting question is whether or not this can this be mitigated
at all, and to what degree.  In particular: can we use automatic
differentiation to <em>implement</em> efficient symbolic differentiation?  Here’s a
possible attack plan:</p>

<ul>
  <li>use an existing automatic differentiation implementation to calculate the
gradient for some target function at a point</li>
  <li>capture the symbolic expression for the gradient and optimize it by 
eliminating common subexpressions or whatnot</li>
  <li>reify that expression as a function that we can evaluate for any input</li>
  <li>voila, (more) efficient gradient</li>
</ul>

<p>Ed Kmett’s <a href="http://hackage.haskell.org/package/ad">‘ad’ library</a> is the
best automatic differentiation library I know of, in terms of its balance
between power and accessibility.  It can be used on arbitrary Haskell types
that are instances of the <code>Num</code> typeclass and can carry out automatic
differentiation via a number of modes, so it’s very easy to get started with.
Rather than rolling my own AD implementation to try to do this sort of thing,
I’d much rather use his.</p>

<p>Start with a really basic language.  In practice we’d be interested in
working with more expressive things, but this is sort of the minimal
interesting language capable of illustrating the problem:</p>

<p>``` haskell
import Numeric.AD</p>

<p>data Expr a =
    Lit a
  | Var String
  | Add (Expr a) (Expr a)
  | Sub (Expr a) (Expr a)
  | Mul (Expr a) (Expr a)
  deriving (Eq, Show)</p>

<p>instance Num a =&gt; Num (Expr a) where
  fromInteger = Lit . fromInteger
  e0 + e1 = Add e0 e1
  e0 - e1 = Sub e0 e1
  e0 * e1 = Mul e0 e1
```</p>

<p>We can already use Kmett’s ad library on these expressions to generate symbolic
expressions.  We just have to write functions we’re interested in generically
(using <code>+</code>, <code>-</code>, and <code>*</code>) and then call <code>diff</code> or <code>grad</code> or whatnot on them
with a concretely-typed argument.  Some examples:</p>

<pre><code>&gt; :t diff (\x -&gt; 2 * x ^ 2) 
diff (\x -&gt; 2 * x ^ 2) :: Num a =&gt; a

&gt; diff (\x -&gt; 2 * x ^ 2) (Lit 1)
Mul (Add (Mul (Lit 1) (Lit 1)) (Mul (Lit 1) (Lit 1))) (Lit 2)

&gt; grad (\[x, y] -&gt; 2 * x ^ 2 + 3 * y) [Lit 1, Lit 2]
[ Add (Lit 0) (Add (Add (Lit 0) (Mul (Lit 1) (Add (Lit 0) (Mul (Lit 2) (Add
  (Lit 0) (Mul (Lit 1) (Lit 1))))))) (Mul (Lit 1) (Add (Lit 0) (Mul (Lit 2) (Add
  (Lit 0) (Mul (Lit 1) (Lit 1)))))))
, Add (Lit 0) (Add (Lit 0) (Mul (Lit 3) (Add (Lit 0) (Mul (Lit 1) (Lit 1)))))
]
</code></pre>

<p>It’s really easy to extract a proper derivative/gradient ‘function’ by
doing something like this:</p>

<pre><code>&gt; diff (\x -&gt; 2 * x ^ 2) (Var "x")
Mul (Add (Mul (Var "x") (Lit 1)) (Mul (Lit 1) (Var "x"))) (Lit 2)
</code></pre>

<p>and then, given that expression, reify a direct function for the derivative by
substituting over the captured variable and evaluating everything.  Here’s some
initial machinery to handle that:</p>

<p>``` haskell
– | Close an expression over some variable.
close :: Expr a -&gt; String -&gt; a -&gt; Expr a
close (Add e0 e1) s x = Add (close e0 s x) (close e1 s x)
close (Sub e0 e1) s x = Sub (close e0 s x) (close e1 s x)
close (Mul e0 e1) s x = Mul (close e0 s x) (close e1 s x)</p>

<p>close (Var v) s x
  | v == s    = Lit x
  | otherwise = Var v</p>

<p>close e _ _ = e</p>

<p>– | Evaluate a closed expression.
eval :: Num a =&gt; Expr a -&gt; a
eval (Lit d) = d
eval (Var _) = error “expression not closed”
eval (Add e0 e1) = eval e0 + eval e1
eval (Sub e0 e1) = eval e0 - eval e1
eval (Mul e0 e1) = eval e0 * eval e1
```</p>

<p>So, using this on the example above yields</p>

<pre><code>&gt; let testExpr = diff (\x -&gt; 2 * x ^ 2) (Var "x")
&gt; eval . close testExpr "x" $ 1
2
</code></pre>

<p>and it looks like a basic <code>toDerivative</code> function for expressions could be
implemented as follows:</p>

<p><code>haskell
-- | Do some roundabout AD.
toDerivative expr = eval . close diffExpr "x" where
  diffExpr = diff expr (Var "x")
</code></p>

<p>But that’s a no go.  ‘ad’ throws a type error, as presumably we’d be at risk of
perturbation confusion:</p>

<pre><code>Couldn't match expected type ‘AD
                                s (Numeric.AD.Internal.Forward.Forward (Expr c))
                              -&gt; AD s (Numeric.AD.Internal.Forward.Forward (Expr c))’
            with actual type ‘t’
  because type variable ‘s’ would escape its scope
This (rigid, skolem) type variable is bound by
  a type expected by the context:
    AD s (Numeric.AD.Internal.Forward.Forward (Expr c))
    -&gt; AD s (Numeric.AD.Internal.Forward.Forward (Expr c))
  at ParamBasic.hs:174:14-32
Relevant bindings include
  diffExpr :: Expr c (bound at ParamBasic.hs:174:3)
  expr :: t (bound at ParamBasic.hs:173:14)
  toDerivative :: t -&gt; c -&gt; c (bound at ParamBasic.hs:173:1)
In the first argument of ‘diff’, namely ‘expr’
In the expression: diff expr (Var "x")
</code></pre>

<p>Instead, we can use ad’s <code>auto</code> combinator to write an alternate eval function:</p>

<p>``` haskell
autoEval :: Mode a =&gt; String -&gt; Expr (Scalar a) -&gt; a -&gt; a
autoEval x expr = (<code>go</code> expr) where
  go _ (Lit d) = auto d
  go v (Var s)
    | s == x    = v
    | otherwise = error “expression not closed”</p>

<p>go v (Add e0 e1) = go v e0 + go v e1
  go v (Sub e0 e1) = go v e0 - go v e1
  go v (Mul e0 e1) = go v e0 * go v e1
```</p>

<p>and using that, implement a working <code>toDerivative</code>:</p>

<p><code>haskell
toDerivative :: Num a =&gt; String -&gt; Expr (Expr a) -&gt; Expr a
toDerivative v expr = diff (autoEval v expr)
</code></p>

<p>which, though it has a weird-looking type, typechecks and does the trick:</p>

<pre><code>&gt; let diffExpr = toDerivative "x" (Mul (Lit 2) (Mul (Var "x") (Var "x"))) (Var "x")
Mul (Add (Mul (Var "x") (Lit 1)) (Mul (Lit 1) (Var "x"))) (Lit 2)

&gt; eval . close "x" 1 $ diffExpr
4
</code></pre>

<p>So now we have access to a reified AST for a derivative (or gradient), which
can be tweaked and optimized as needed.  Cool.</p>

<p>The available optimizations depend heavily on the underlying language.  For
starters, there’s easy and universal stuff like this:</p>

<p>``` haskell
– | Reduce superfluous expressions.
elimIdent :: (Num a, Eq a) =&gt; Expr a -&gt; Expr a
elimIdent (Add (Lit 0) e) = elimIdent e
elimIdent (Add e (Lit 0)) = elimIdent e
elimIdent (Add e0 e1)     = Add (elimIdent e0) (elimIdent e1)</p>

<p>elimIdent (Sub (Lit 0) e) = elimIdent e
elimIdent (Sub e (Lit 0)) = elimIdent e
elimIdent (Sub e0 e1)     = Sub (elimIdent e0) (elimIdent e1)</p>

<p>elimIdent (Mul (Lit 1) e) = elimIdent e
elimIdent (Mul e (Lit 1)) = elimIdent e
elimIdent (Mul e0 e1)     = Mul (elimIdent e0) (elimIdent e1)</p>

<p>elimIdent e = e
```</p>

<p>Which lets us do some work up-front:</p>

<pre><code>&gt; let e = 2 * Var "x" ^ 2 + Var "x" ^ 4
Add (Mul (Lit 2) (Mul (Var "x") (Var "x"))) (Mul (Mul (Var "x") (Var "x"))
(Mul (Var "x") (Var "x")))

&gt; let ge = toDerivative "x" e
Add (Mul (Add (Mul (Var "x") (Lit 1)) (Mul (Lit 1) (Var "x"))) (Lit 2))
(Add (Mul (Mul (Var "x") (Var "x")) (Add (Mul (Var "x") (Lit 1)) (Mul
(Lit 1) (Var "x")))) (Mul (Add (Mul (Var "x") (Lit 1)) (Mul (Lit 1)
(Var "x"))) (Mul (Var "x") (Var "x"))))

&gt; let geOptim = elimIdent ge
Add (Mul (Add (Var "x") (Var "x")) (Lit 2)) (Add (Mul (Mul (Var "x")
(Var "x")) (Add (Var "x") (Var "x"))) (Mul (Add (Var "x") (Var "x")) (Mul
(Var "x") (Var "x"))))

&gt; eval . close "x" 1 $ geOptim
8
</code></pre>

<p>But there are also some more involved optimizations that can be useful for some
languages.  The basic language I’ve been using above, for example, has no
explicit support for sharing common subexpressions.  You’ll recall from <a href="/blog/blog/2014/05/30/sharing-in-haskell-edsls/">one of
my previous posts</a> that we
have a variety of methods to do that in Haskell EDSLs, including some that
allow sharing to be observed without modifying the underlying language.  We can
use <code>data-reify</code>, for example, to observe any implicit sharing in expressions:</p>

<pre><code>&gt; reifyGraph $ geOptim
let [(1,AddF 2 6),(6,AddF 7 10),(10,MulF 11 12),(12,MulF 4 4),(11,AddF 4 4),
(7,MulF 8 9),(9,AddF 4 4),(8,MulF 4 4),(2,MulF 3 5),(5,LitF 2),(3,AddF 4 4),
(4,VarF "x")] in 1
</code></pre>

<p>And even make use of a <a href="http://hackage.haskell.org/package/data-reify-cse">handy library</a> found on Hackage
for performing common subexpression elimination on graphs returned by
<code>reifyGraph</code>:</p>

<pre><code>&gt; cse &lt;$&gt; reifyGraph geOptim
let [(5,LitF 2),(1,AddF 2 6),(3,AddF 4 4),(6,AddF 7 10),(2,MulF 3 5),
(10,MulF 3 8),(8,MulF 4 4),(7,MulF 8 3),(4,VarF "x")] in 1
</code></pre>

<p>With an appropriate <a href="/blog/blog/2014/05/30/sharing-in-haskell-edsls/">graph evaluator</a>
we can cut down the size of the syntax we have to traverse substantially.</p>

<p>Happy automasymbolic differentiating!</p>

]]></content>
  </entry>
  
</feed>
