<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: probability | jtobin.ca]]></title>
  <link href="http://jtobin.ca/blog/categories/probability/atom.xml" rel="self"/>
  <link href="http://jtobin.ca/"/>
  <updated>2014-05-31T18:10:34+10:00</updated>
  <id>http://jtobin.ca/</id>
  <author>
    <name><![CDATA[Jared Tobin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sampling Functions and Generative Models]]></title>
    <link href="http://jtobin.ca/blog/2014/03/17/sampling-functions-vs-generative-models/"/>
    <updated>2014-03-17T08:20:55+10:00</updated>
    <id>http://jtobin.ca/blog/2014/03/17/sampling-functions-vs-generative-models</id>
    <content type="html"><![CDATA[<p>The term ‘probability distribution’ is actually pretty vague.  A probability
distribution is an abstract object that can be distinguished uniquely (i.e.
from other probability distributions) by way of any of a bunch of concrete
representations.  Probability density or mass functions, moment generating
functions, characteristic functions, cumulative distribution functions, random
variables, and measures all reify a probability distribution as something
tangible that can be worked with.  Image measures in particular are sometimes
called ‘distributions’, though they still just form a single possible
reification of the underlying concept.  In formal probability theory the term
‘law’ is often used to refer to the abstract object being characterized.</p>

<p>Different characterizations are useful when doing different kinds of work. 
Measures are useful when doing proofs.  Probability densities and mass
functions are useful for a whole whack of applications.  Moment generating
functions are useful for exercises in introductory mathematical statistics
courses.</p>

<p>Sampling functions - procedures that produce samples from some target
distribution - also characterize probability distributions uniquely.  They are
an excellent basis for introducing and transforming uncertainty in
probabilistic programming languages.  A sampling function takes as its sole
input a <em>stream of randomness</em>, which is consumed and transformed to produce a
sample from the distribution that it characterizes.  The stream can be a lazy
list or, similarly, a pseudo-random number generator.</p>

<p>Sampling functions are precursors to <em>generative models</em>: models that take both
randomness and causal inputs as arguments and produce a possible effect.
Generative models are the main currency of probabilistic programming; they
specify a mapping between hypothesized causes and a probability distribution
over effects.  Sampling functions are the basis for handling all uncertainty in
several PP implementations.</p>

<p>It’s useful to look at sampling functions and generative models to emphasize
the distinction between the two.  In Haskelly pseudocode, they have the
following types:</p>

<p>``` haskell
samplingFunction :: Randomness -&gt; Effect</p>

<p>generativeModel  :: Causes -&gt; Randomness -&gt; Effect
```</p>

<p>It’s easy to see that a generative model, when provided with some causes, is
itself a sampling function.</p>

<p>We can use a monad to abstract out the provisioning of randomness and make
everything a little cleaner.  Imagine ‘Observable’ to be a monad that handles
the propagation of randomness in our functions; any type tagged with
‘Observable’ is a probability distribution over some other type (and maybe we
would run it with a function called <code>observe</code> or <code>sample</code>).  Using that, we can
write the above as follows:</p>

<p>``` haskell
samplingFunction :: Observable Effect</p>

<p>generativeModel  :: Causes -&gt; Observable Effect
```</p>

<p>Very clean.  Here it’s immediately clear that only difference between a
sampling function and a model is the introduction of causes to the latter.  A
generative model contains some internal logic that manipulates external causes
in some way; a sampling function does not.</p>

<p>You can see some in-progress development of this idea
<a href="http://github.com/jtobin/observable">here</a> and
<a href="http://github.com/glutamate/probably-baysig">here</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Measures and Continuations]]></title>
    <link href="http://jtobin.ca/blog/2014/01/08/measures-and-continuations/"/>
    <updated>2014-01-08T13:32:05+10:00</updated>
    <id>http://jtobin.ca/blog/2014/01/08/measures-and-continuations</id>
    <content type="html"><![CDATA[<p>I’ve always been interested in measure theory.  I’m not even sure why, exactly.
Probably because it initially seemed so mysterious to me.  As an undergrad,
measure theory was this unknown, exotic key to <em>truly</em> understanding
probability.</p>

<p>Well, sort of.  It’s certainly the key to understanding formal probability,
but it no longer seems all that exotic, nor really necessary for grokking what
I’d call the true essence of probability.  It’s pretty much real analysis with
specialized attention paid to notions of factoring (independence) and ratios
(conditioning).  Nowadays I relate it moreso to accounting; not the most
inspiring of subjects, but necessary if you want to make sure everything adds
up the way you think it should.</p>

<!-- more -->

<p>Measures are just one of many structures that uniquely characterize probability
distributions, such as density functions, Fourier transforms, sampling
functions, and so on.  They’ve been considered to be particularly useful for
proving things, due to their generality.  They can be pretty abstract, but a
perfectly suitable analogy of a measure for most practical purposes is a
function that, when provided with a region, returns a volume.  Pretty simple.</p>

<p>Volume also tends to be synonymous with integration, in that given a (positive)
function, the integral of that function over some region defines a volume, or
‘area under the curve’.  For more general functions, integrals can be thought
of as returning a kind of center of mass.</p>

<p>Integration and measures go hand in hand.  In abstract integration, one
integrates a function over some region ‘against’ a measure, which assigns
particular weights to infinitesimals of the underlying space.  Measures that
tend to pop up in practice tend to be counting measure, Lebesgue measure, and
assorted image measures corresponding to particular probability distributions.</p>

<p>Imagine representing a measure on a computer.  One possible way to do it is to
represent a measure as an integral directly.  Or, rather, as a generic
<em>procedure</em> for integration.</p>

<p>Consider the setup where you pass a measurable function to an integration
procedure and it spits out a center of mass.  This has the convenient benefit
that the responsibility for preserving measurability can be delegated to the
function passed as an argument, rather than hard-coded into a measure itself
(which might be very hard to do).  Implemented in a type system - take
Haskell’s - we can implement a measure as something like</p>

<pre><code>newtype Measure a = Measure { measure :: (a -&gt; Double) -&gt; Double }
</code></pre>

<p>which is a wrapper over a measurable function that we want to integrate.</p>

<p>This is a restricted form of the <code>Cont</code> type used to define the continuation
monad.  That makes sense; we’re defining a measure to be a program that should
perform integration, and the particular implementation of that program defines
a particular measure.  If we’re willing to play things kind of fast and loose,
we can define measures by continuations proper:</p>

<pre><code>newtype Cont r a = Cont { runCont :: (a -&gt; r) -&gt; r }

instance Functor (Cont r) where
  fmap f c = Cont $ \g -&gt; runCont c (g . f)

instance Monad (Cont r) where
  return x = Cont $ \f -&gt; f x
  c &gt;&gt;= f  = Cont $ \d -&gt; runCont c $ \g -&gt; runCont (f g) d

type Measure a = Cont Double a
</code></pre>

<p>There are three immediately interesting things about this setup:</p>

<ul>
  <li><code>fmap</code> creates an image or pushforward measure by pushing a function onto an
existing measure</li>
  <li><code>return</code> is the Dirac measure at a point</li>
  <li><code>&gt;&gt;=</code> is a marginalizing operator</li>
</ul>

<p>There’s a big caveat in going this route: nonlinear continuation-y stuff like
<code>callCC</code> will break everything, so you can’t use it in the context of measures.
In practice defining a specific restricted type might be a bit safer, but we
can just avoid using it.</p>

<p>Usually we have something like $ \int f d\mathbb{P}$ in mathematical notation
to denote the integral of a function $ f $ against a measure $\mathbb{P}$, so
a more familiar version of <code>runCont</code> is</p>

<pre><code>integrate :: (a -&gt; Double) -&gt; Measure a -&gt; Double
integrate = flip runCont
</code></pre>

<p><code>integrate</code> defines the most basic ‘query’ of sorts on a measure.  You can
query a measure to find its volume, center of mass, and various higher-ordere
moments.  The simplest examples include:</p>

<pre><code>volume :: Measure a -&gt; Double
volume = integrate (const 1)

expectation :: Measure a -&gt; Double
expectation = integrate id

variance :: Measure a -&gt; Double
variance mu = integrate (^ 2) mu - expectation mu ^ 2
</code></pre>

<p>For any probability measure, <code>volume</code> should trivially return 1.</p>

<p>To create measures, you can put together functions like</p>

<pre><code>fromDensityCounting :: (a -&gt; Double) -&gt; [a] -&gt; Measure a
fromDensityLebesgue :: (Double -&gt; Double) -&gt; Measure Double
fromObservations    :: [a] -&gt; Measure a
</code></pre>

<p>that will create measures from the appropriate underlying structures. 
<code>fromObservations</code> is probably the most interesting, being the least
restrictive of the three.  You could also implement <code>fromSamplingFunction</code> to
create a (random) measure from a sampling function and a seed.</p>

<p>It’s easy to define stuff like abstract addition, subtraction, and
multiplication on measures.  Just use <code>liftM2</code> and your arithmetic operator of
choice.  In particular, abstract addition of measures corresponds to
measure convolution.</p>

<p>Viewing measures in this way is kind of neat, if of questionable practical
value.  You can see a implementation of this stuff
<a href="http://github.com/jtobin/measurable">here</a>, which includes a few examples to
boot.  But in practice, you can’t seem to get very far with an implementation
like this.  Exact computation involving probabilities is always going to be
limited to toy cases.  One of the key morals of <a href="http://web.mit.edu/vkm/www/vkm-dissertation.pdf">Vikash Mansinghka’s excellent
dissertation</a>, relevant here,
is “don’t compute probabilities; instead, sample good guesses”.  </p>

]]></content>
  </entry>
  
</feed>
